{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Models\n",
    "Two different ways to train linear regression models:\n",
    "1. Direct \"closed form\" equation that directly computes the model parameters that best fit the model to the training set (i.e. the model parameters that minimise the cost function over the training set)\n",
    "2. Use an iterative operation called _gradient descent_ that gradually tweaks the model parameters to minimise the cost function eventually converging to the same set of parameters as the first method.\n",
    "\n",
    "Polynomial regression is a more complex model that can fit non-linear datasets. Since the model has more parameters than Linear Regression, it is more prone to overfitting the data. To detect whether this is the case, _learning curves_ are used along with _regularisation techniques_ to reduce the risk of overfitting.\n",
    "\n",
    "Two common models used for classification are: Logistic Regression and Softmax Regression.\n",
    "\n",
    "### Linear Regression\n",
    "The first linear regression model of life satisfaction was given by the equation \n",
    "\n",
    "$$life\\_satisfaction = \\theta_{0} + \\theta_{1} \\times GDP\\_per\\_capita$$ \n",
    "\n",
    "The model is a linear function of the input feature GDP_per_capita. $\\theta_{0}$ and $\\theta_{1}$ are the model's parameters. The linear model computes a weighted sum of the input features plus a constant called the _bias term_ (or the _intercept term_).\n",
    "\n",
    "$$\\hat{y} = \\theta_{0} + \\theta_{1}x_{1} + \\theta_{2}x_{2} + ... + \\theta_{n}x_{n}$$\n",
    "\n",
    "Where $\\hat{y}$ is the predicted value, n is the number of features, $x_{i}$ is the $i^{th}$ feature value, $\\theta_{j}$ is the $j^{th}$ model parameter (including the bias term $\\theta_{0}$ and the feature weights $\\theta_{1}, \\theta_{2}, ..., \\theta_{n}$)\n",
    "\n",
    "The equivalent vectorised form of the above equation is\n",
    "\n",
    "$$\\hat{y} = h_{\\theta}(x) = \\mathbf{\\theta} \\bullet \\mathbf{x} $$\n",
    "\n",
    "Where $\\theta$ is the model's _parameter vector_, containing the bias term $\\theta_{0}$ and the feature weights $\\theta_{1}$ to $\\theta_{n}$. $\\mathbf{x}$ is the instance's _feature vector_ containing $x_{0}$ to $x_{n}$, with $x_{0}$ always equal 1. $\\theta \\bullet \\mathbf{x}$ is the dot product of the vectors $\\theta$ and $\\mathbf{x}$ which is equal to $\\theta_{0}x_{0} + \\theta_{1}x_{1} + \\theta_{2}x_{2} + ... + \\theta_{n}x_{n}$ and $h_{\\theta}$ is the _hypothesis function_ using the model parameters, $\\theta$.\n",
    "\n",
    "Note that in machine learning, vectors are usually stored as _column vectors_ so getting the prediction requires $\\theta$ to be transposed $\\hat{y} = \\theta^{T}\\mathbf{x}$.\n",
    "\n",
    "A reminder that linear regression model is evaluated by the Root Mean Square Error and so to train a linear regression model, the model parameters should be computed such that it minimises RMSE. In practice, the MSE is easier to minimise than the RMSE and can lead to the same result (minising the MSE also minimises its square root).\n",
    "\n",
    "The MSE of a Linear Regression model with hypothesis $h_{\\theta}$ on a training set $\\mathbf {X}$ is calculated by:\n",
    "$$ MSE(\\mathbf{X}, h_{\\theta}) = \\frac{1}{m} \\sum^{m}_{i=1} (\\theta^{T}\\mathbf{x}^{i}-y^{i})^2$$\n",
    "\n",
    "### The Normal Equation\n",
    "To find the value of $\\theta$ that minimises the cost function, the _closed-form solution_ (the mathematical equation that gives the result directly) is called the _Normal Equation_.\n",
    "$$ \\hat{\\theta} = (\\mathbf{X}^{T} \\mathbf{X})^{-1} \\mathbf{X}^{T} \\mathbf{y} $$\n",
    "Where $\\hat{\\theta}$ is the value of $\\theta$ that minimises the cost function and $\\mathbf{y}$ is the vector of target values containing $y^{1}$ to $y^{m}$.\n",
    "\n",
    "To generate some linear looking data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X = 2 * np.random.rand(100, 1)\n",
    "y = 4 + 3 * X + np.random.randn(100, 1)\n",
    "\n",
    "plt.plot(X, y, \"b.\")\n",
    "plt.xlabel(\"$x_1$\", fontsize=18)\n",
    "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
    "plt.axis([0, 2, 0, 15])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute $\\hat{\\mathbf{\\theta}}$ using the Normal Equation. Use the inv() function from NumPy's linear algebra module to compute the inverse of the matrix and use the dot() method for matrix multiplication:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.85272306],\n",
       "       [3.09392767]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_b = np.c_[np.ones((100, 1)), X] #add x0 = 1 to each instance\n",
    "theta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)\n",
    "theta_best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The actual function used to generate noise is $y = 4 + 3x_{1} + $ Gaussian noise. The perfect answer would have been $\\theta_{0} = 4$ and $\\theta_{1} = 3$ instead of $\\theta_{0} = 3.805$ and $\\theta_{1} = 3.405$ but the noise made it impossible to recover the original function.\n",
    "\n",
    "To make a prediction using the value of $\\hat{\\theta}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.85272306],\n",
       "       [10.0405784 ]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new = np.array([[0], [2]])\n",
    "X_new_b = np.c_[np.ones((2,1)), X_new] #add x0 = 1 to each instance\n",
    "y_predict = X_new_b.dot(theta_best)\n",
    "y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plot the model's predictions\n",
    "plt.plot(X_new, y_predict, \"r-\", linewidth=2, label=\"Predictions\")\n",
    "plt.plot(X, y, \"b.\")\n",
    "plt.xlabel(\"$x_1$\", fontsize=18)\n",
    "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
    "plt.legend(loc=\"upper left\", fontsize=14)\n",
    "plt.axis([0, 2, 0, 15])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([3.85272306]), array([[3.09392767]]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Performing linear regression using Scikit-Learn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X, y)\n",
    "lin_reg.intercept_, lin_reg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.85272306],\n",
       "       [10.0405784 ]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_reg.predict(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LinearRegression class is based on the scipy.linalg.lstsq() function which can be called directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.85272306],\n",
       "       [3.09392767]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta_best_svd, residuals, rank, s = np.linalg.lstsq(X_b, y, rcond=1e-6)\n",
    "theta_best_svd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function computes $\\hat{\\theta} = \\mathbf{X}^{+}\\mathbf{y}$, where $\\mathbf{X}^+$ is the _pseudoinverse_ of $\\mathbf{X}$ (specifically the Moore-Penrose inverse) which can be computed using np.linalg.pinv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.85272306],\n",
       "       [3.09392767]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.pinv(X_b).dot(y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pseudoinverse is computed using standard matrix factorisation technique called _Singular Value Decomposition_ (SVD) that can decompose the training set matrix $\\mathbf{X}$ into the matrix multiplication of three matrices $\\mathbf{U, \\Sigma, V^T}$. The pseudoinverse is $\\mathbf{X^+=V\\Sigma^+ U^T}$. \n",
    "\n",
    "To compute the matrix $\\Sigma^+$, the algorithm takes $\\Sigma$ and sets to zero all values smaller than a tiny threshold value, then it replaces all the non-zero values with their inverse and finally transposes the resulting matrix. This approach is more efficient than computing the Normal Equation and handles edge cases nicely. The normal equation might not work if the matrix $\\mathbf{X^T X}$ is not invertible (i.e. singular) such as if $m < n$ or if some features are redundant, but the pseudoinverse is always defined.\n",
    "\n",
    "### Computational Complexity\n",
    "The Normal Equation computes the inverse of $\\mathbf{X^T X}$ which is an $(n+1) \\times (n+1)$ matrix (where n is the number of features). The _computational complexity_ of inverting such a matrix is typically about $O(n^{2.4})$ to $O(n^3)$ depending on the implementation, i.e. if the features are doubled then the computational time increases by $2^3.4 = 5.3$ to $2^3 = 8$ times.\n",
    "\n",
    "The SVD approach used by Scikit-Learn's LinearRegression class is about $O(n^2)$. \n",
    "\n",
    "Both the SVD and Normal Equation approach are very slow when the number of features is large (~100,000).\n",
    "\n",
    "However, once the Linear Regression model is trained (using Normal Equation or other algorithms), predictions are very fast; the computational complexity is linear with respect to both the number of instances you want to make predictions on and the number of features. In other words, making predictions on twice as many instances (or twice as many features) takes about twice as much time.\n",
    "\n",
    "### Gradient Descent\n",
    "_Gradient Descent_ is a generic optimisation algorithm capable of finding optimal solutions to a range of problems. It works by tweaking the parameters iteratively to minimise a cost function and is suitable for larger datasets.\n",
    "\n",
    "Gradient descent measures the local gradient of the error function with respect to the parameter vector $\\mathbf{\\theta}$ and it goes in the direction of descending gradient. Once the gradient reaches 0, it is at a minimum.\n",
    "\n",
    "The initial $\\mathbf{\\theta}$ is filled with random values known as _random initialisation_ and then the model is improved gradually by taking small steps at a time trying to decrease the cost function (e.g. the MSE) until the algorithm converged at a minimum.\n",
    "\n",
    "<img src=\"Figure%204-3.PNG\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "\n",
    "An important parameter in gradient descent is the size of the steps, determined by the _learning rate_ hyperparameter. If the learning rate is too small, the algorithm will have to go through many steps to converge which takes a long time.\n",
    "\n",
    "<img src=\"Figure%204-4.PNG\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "\n",
    "Alternatively, if the learning rate is too large, the algorithm might climb to larger and larger values and diverge without finding a good solution.\n",
    "\n",
    "<img src=\"Figure%204-5.PNG\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "\n",
    "Finally, the terrain of the cost function is not always a bowl. There might be holes, ridges or plateaus making convergence at the minimum difficult. Figure 4.6 illustrates common problems with gradient descent: if the random initialisation starts the algorithm on the left, the algorithm will converge at a _local minimum_ which is not as optimal as the _global minimum_. If it initialises on the right, it has to cross a plateau which if the algorithm stops too early, it will never converge at the global minimum.\n",
    "\n",
    "<img src=\"Figure%204-6.PNG\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "\n",
    "Fortunately, the MSE cost function for a Linear Regression model is a *convex function*, i.e. any line segment drawn between two points chosen on the curve never crosses the curve. This means that the function has no local minima just one global minimum. It is also a continuous function with a slope that never changes abruptly (the derivative is *Lipschitz continuous*). This implication is that gradient descent is guaranteed to converge close to the global minimum given enough time and a sufficiently high learning rate.\n",
    "\n",
    "The cost function has the shape of a bowl but can be elongated if features have vastly different scales. Figure 4.7 shows gradient descent on a training set where features 1 and 2 have the same scale (left) compared to when the training set has a feature 1 has much smaller values than feature 2 (right).\n",
    "\n",
    "<img src=\"Figure%204-7.PNG\" alt=\"Drawing\" style=\"width: 800px;\"/>\n",
    "\n",
    "On the left, the gradient descent algorithm goes straight towards the minimum, thereby reaching it quickly, On the right, the first direction goes almost orthogonal to the direction of the global minimum then approaches down a flat valley eventually to a minimum which can take a long time. Therefore, when using gradient descent the features should have a similar scale to avoid this problem.\n",
    "\n",
    "The algorithm searches in the model's _parameter space_ so the more parameters the model has, the more dimensions the algorithm has to explroe before reaching the global minimum.\n",
    "\n",
    "### Batch Gradient Descent\n",
    "\n",
    "To implement gradient descent, compute the gradient of the cost function with respect to each model parameter $\\theta_{j}$. The partial derivative of the cost function with respect to the parameter $\\theta_{j}$, $\\frac{\\partial}{\\partial \\theta_{j}}MSE(\\theta)$\n",
    "$$\\frac{\\partial}{\\partial \\theta_{j}}MSE(\\theta) = \\frac{2}{m}\\sum^m_{i=1} (\\theta ^T \\mathbf{x}^i - y^i)x_j^i$$\n",
    "\n",
    "Instead of computing the partial derivatives individually, the gradient vector of the cost function can compute all the partial derivatives in one equation. The gradient of the vector, $\\nabla_{\\theta}MSE(\\mathbf{\\theta})$, contains all the partial derivatives of the cost function (one for each model parameter).\n",
    "\n",
    "<img src=\"Equation%204.6.PNG\" alt=\"Drawing\" style=\"width: 300px;\"/>\n",
    "\n",
    "The equation shows that the formula performs calculations over the full training set, __X__, at each gradient descent step, but scales well with the number of features. Therefore, implementing gradient descent on a large training set is slow but tends to perform well with large number of features.\n",
    "\n",
    "Once the gradient vector that goes uphill is found, go in the opposite direction to go downhill, i.e. subtract by $\\nabla_\\theta MSE(\\mathbf{\\theta})$ from $\\mathbf{\\theta}$. This is where the _learning rate_, $\\eta$, is introduced to multiply the gradient vector by to determine the size of the downhill step.\n",
    "\n",
    "$$\\mathbf{\\theta}^{(next \\space step)} = \\mathbf{\\theta} - \\eta \\nabla_\\theta MSE(\\theta)$$\n",
    "\n",
    "To implement the algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.85272306],\n",
       "       [3.09392767]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eta = 0.1 # learning rate\n",
    "n_iterations = 1000\n",
    "m = 100\n",
    "\n",
    "theta = np.random.randn(2, 1) #random initialisation\n",
    "\n",
    "for iteration in range(n_iterations):\n",
    "    gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y)\n",
    "    theta = theta - eta * gradients\n",
    "\n",
    "theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Figure%204-8.PNG\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "The left case illustrates when the learning rate is too low and the algorithm takes a long time to converge to the global minimum. The middle case is a good learning rate, where convergence happens after a few iterations. The case on the right is when the learning rate is too high and the algorithm diverges and never reaches the minimum.\n",
    "\n",
    "To find a good learning rate, grid search can be used. However, the number of iterations in the grid search should be limited to stop the grid search if the learning rate causes divergence.\n",
    "\n",
    "The number of iterations set to run the algorithm should interrupt the algorithm when the gradient vector becomes tiny - that is when the norm becomes smaller than a tiny number, $\\varepsilon$ called the _tolerance_, because this is the point when the Gradient Descent has (almost) reached the minimum.\n",
    "\n",
    "### Convergence Rate\n",
    "\n",
    "As the cost function is convex and its slope does not change abruptly (for MSE cost function), Batch Gradient Descent with a fixed learning rate will eventually converge to the optimal solution, but it could take a long time to compute: it can take $O(1/\\varepsilon)$ iterations to reach the optimal solution. If the tolerance is divided by 10, the algorithm will take 10 times as long to run.\n",
    "\n",
    "### Stochastic Gradient Descent\n",
    "The main problem is that Batch Gradient Descent uses the whole training set to compute the gradients at every step and so becomes slow with large training sets. The opposite extreme is _Stochastic Gradient Descent_ picks a random instance in the training set at every step and computes the gradients based on that single instance. SGD can be implemented as an out of core algorithm as only a single instance has to be in memory at a time. This also means that SGD is much faster as there is very little data being manipulated for each iteration.\n",
    "\n",
    "However, the stochastic (random) nature of the algorithm makes is less regular than Batch Gradient Descent: instead of gently decreasing the gradient until a minimum is reached, the function will oscillate and only decreasing the gradient on average. Over time it will still end up very close to the minimum but will continue to oscillate even at the minimum so the algorithm stops at a good parameter value but not the optimal one.\n",
    "\n",
    "<img src=\"Figure%204-9.PNG\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "\n",
    "For cost functions with irregular terrains like the one in Figure 4-6, the algorithm can jump out of local minima towards the global minimum due to its random nature which cannot be done for Batch Gradient Descent.\n",
    "\n",
    "Randomness can be used to escape local optima but is bad because the algorithm never setttles at the minimum. The solution is to gradually decrease the learning rate. Start with a large learning rate to make quick progress and escape local minima but tune it to go smaller and smaller to allow the algorithm to settle near the global minimum. The process is known as _stimulated annealing_. The function determines the learning rate at each iteration called the _learning schedule_. If the learning rate is reduced too quickly, the algorithm can still get trapped at a local minima but if the learning rate is reduced too slow, the computational time will be long.\n",
    "\n",
    "The following example implements SGD using a simple learning schedule:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.85596386],\n",
       "       [3.03597422]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_epochs = 50\n",
    "t0, t1 = 5, 50 # learning schedule hyperparameters\n",
    "\n",
    "def learning_schedule(t):\n",
    "    return t0/(t + t1)\n",
    "\n",
    "theta = np.random.randn(2, 1) # random initialisation\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for i in range(m):\n",
    "        random_index = np.random.randint(m)\n",
    "        xi = X_b[random_index:random_index+1]\n",
    "        yi = y[random_index:random_index+1]\n",
    "        gradients = 2 * xi.T.dot(xi.dot(theta) - yi)\n",
    "        eta = learning_schedule(epoch * m + i)\n",
    "        theta = theta - eta * gradients\n",
    "\n",
    "theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By convention, the algorithm iterates by rounds of m iterations; each round is called an _epoch_. While Batch Gradient Descent iterated 1000 times through the whole training set, this code goes through the training set 50 times to reach a good solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
